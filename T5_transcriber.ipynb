{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI9NrgXj8f35"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets\n",
        "!pip install transformers[torch]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import pymupdf\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from google.colab import userdata, drive\n",
        "\n"
      ],
      "metadata": {
        "id": "4TpWoqOL8q96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/SLPA/input_output.csv', delimiter='\\t')\n",
        "\n",
        "# Convert the dataframe to a Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(data)\n",
        "\n",
        "# Initialize the tokenizer and model\n",
        "model_name = 't5-small'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-hjX9pN8s0c",
        "outputId": "bb8b2c9c-bd50-411f-a73d-b60b7f4096b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = ['JSONify: '+ex for ex in examples['description']]\n",
        "    targets = [ex for ex in examples['json']]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "    # Tokenize targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "geH6eyyq8uqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and validation sets\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = split['train']\n",
        "eval_dataset = split['test']"
      ],
      "metadata": {
        "id": "x4IvQW-i8xJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset.set_format(type=\"torch\", columns=['input_text', 'target_text'])\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
        "#valid_dataloader = DataLoader(dataset['validation'], batch_size=4)\n",
        "test_dataloader = DataLoader(eval_dataset, batch_size=4)"
      ],
      "metadata": {
        "id": "93GccVJhah--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/SLPA/results',          # output directory\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    per_device_train_batch_size=8,   # batch size for training\n",
        "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='/content/drive/MyDrive/SLPA/logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",     # evaluate each epoch\n",
        "    save_strategy=\"epoch\"            # save each epoch\n",
        ")\n"
      ],
      "metadata": {
        "id": "tEjaNYIm805C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=eval_dataset,           # evaluation dataset\n",
        ")"
      ],
      "metadata": {
        "id": "YHKsmzcK84at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "wb-5vlP785t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/drive/MyDrive/SLPA/results/trained_model'\n",
        "tokenizer_save_path = '/content/drive/MyDrive/SLPA/results/trained_model'\n",
        "\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(tokenizer_save_path)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "SwqI2rGIc5fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "# Generate predictions on the validation set\n",
        "predictions = trainer.predict(eval_dataset)\n",
        "\n",
        "# Convert predictions to text\n",
        "decoded_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n",
        "\n",
        "# Optionally, you can also convert the labels to text for comparison\n",
        "decoded_labels = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)\n",
        "\n",
        "# Print some example predictions and their corresponding labels\n",
        "for i in range(5):\n",
        "    print(f\"Input: {eval_dataset[i]['input_text']}\")\n",
        "    print(f\"Prediction: {decoded_preds[i]}\")\n",
        "    print(f\"Actual: {decoded_labels[i]}\")\n",
        "    print(\"------\")"
      ],
      "metadata": {
        "id": "ZXxlTMI0c13N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "  model_load_path = '/content/drive/MyDrive/SLPA/results/trained_model'\n",
        "  tokenizer = T5Tokenizer.from_pretrained(model_load_path)\n",
        "  model = T5ForConditionalGeneration.from_pretrained(model_load_path)\n",
        "  return model, tokenizer\n"
      ],
      "metadata": {
        "id": "7nWyRjMQKwZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, tokenizer, input_text):\n",
        "  #input_text = \"Show me all users who joined after January 1st, 2020\"\n",
        "  input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
        "\n",
        "  # Generate the output\n",
        "  output_ids = model.generate(input_ids, num_beams=4, early_stopping=True)\n",
        "  output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "  return output_text"
      ],
      "metadata": {
        "id": "eH0J-aTALdx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = load_model()"
      ],
      "metadata": {
        "id": "EbZjrtliMBjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = inference(model, tokenizer, \"summarize: C shaped hands raised and moving forward\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "-4B3YO-_MIVt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}