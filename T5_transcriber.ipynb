{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI9NrgXj8f35"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets\n",
        "!pip install transformers[torch]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4TpWoqOL8q96"
      },
      "outputs": [],
      "source": [
        "#import pymupdf\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
        "from google.colab import userdata, drive\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f-hjX9pN8s0c"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/SLPA/description_to_handshape.csv', delimiter=',')\n",
        "dataset = Dataset.from_pandas(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 't5-small'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "RPDjNFozf5md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "geH6eyyq8uqd"
      },
      "outputs": [],
      "source": [
        "def preprocess(examples):\n",
        "    inputs = ['Summarize: '+ex for ex in examples['input_text']]\n",
        "    targets = [ex for ex in examples['target_text']]\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    # Tokenize targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_encoding = tokenizer(targets, truncation=True, padding=True, max_length=512)\n",
        "    labels = target_encoding.input_ids\n",
        "\n",
        "    model_inputs['labels'] = labels\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4IvQW-i8xJp"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and validation sets\n",
        "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
        "split = tokenized_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = split['train']\n",
        "eval_dataset = split['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "93GccVJhah--"
      },
      "outputs": [],
      "source": [
        "dataset.set_format(type=\"torch\", columns=['input_text', 'target_text'])\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
        "test_dataloader = DataLoader(eval_dataset, batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEjaNYIm805C"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/SLPA/results',          # output directory\n",
        "    num_train_epochs=5,              # number of training epochs\n",
        "    per_device_train_batch_size=8,   # batch size for training\n",
        "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='/content/drive/MyDrive/SLPA/logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",     # evaluate each epoch\n",
        "    #save_strategy=\"epoch\",            # save each epoch, this quickly uses up space!\n",
        "    learning_rate=1e-4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YHKsmzcK84at"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=eval_dataset,           # evaluation dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wb-5vlP785t_"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "# This step takes up to 4 hours, don't let your computer fall asleep!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwqI2rGIc5fz"
      },
      "outputs": [],
      "source": [
        "model_save_path = '/content/drive/MyDrive/SLPA/results/trained_model'\n",
        "tokenizer_save_path = '/content/drive/MyDrive/SLPA/results/trained_model'\n",
        "\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(tokenizer_save_path)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nWyRjMQKwZg"
      },
      "outputs": [],
      "source": [
        "def load_model(name):\n",
        "  model_load_path = f'/content/drive/MyDrive/SLPA/results/{name}'\n",
        "  tokenizer = T5Tokenizer.from_pretrained(model_load_path)\n",
        "  model = T5ForConditionalGeneration.from_pretrained(model_load_path)\n",
        "  return model, tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH0J-aTALdx8"
      },
      "outputs": [],
      "source": [
        "def inference(model, tokenizer, input_text):\n",
        "  #input_text = \"Show me all users who joined after January 1st, 2020\"\n",
        "  input_text = 'summarize: '+input_text\n",
        "  input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
        "\n",
        "  # Generate the output\n",
        "  output_ids = model.generate(input_ids, num_beams=4, early_stopping=True)\n",
        "  output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "  return output_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tests = [\"horizontal right extended 'A' hand palm down is placed on the facing palm of the left extended 'B' the right thumb is drawn downward across the left palm\",\n",
        "         \"right extended '8' hand palm facing up fingers pointing forward is moved from left to right in a series of successive short arcs\",\n",
        "         \"vertical left extended 'B' hand is held in a fixed position with palm facing right. Vertical right one hand palm forward is placed against the left palm and is pushed upward\",\n",
        "         \"right flat '0' hand is held with fingertips touching or close to the centre of the forehead while left flat '0' hand is held farther forward and slightly lower\"]"
      ],
      "metadata": {
        "id": "_LsROIDfnCv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4B3YO-_MIVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6497163-56d0-47b3-caa8-f7fe08316725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "right hand extended 'A' left hand.\n"
          ]
        }
      ],
      "source": [
        "for t in tests:\n",
        "  output = inference(model, tokenizer, f\"summarize: {t}\")\n",
        "  print(output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}